---
title: "Yackm lab notes"
author: "Ernesto Carrella"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r,echo=FALSE,warning=FALSE}
rawdataFolder<-
  file.path("~","code","lancaster","docs","yackm","rawdata")
```

#Convergence Speed between Marshallian and Keynesian

### Date 

2015-02-08

###Background Information

Fixed wages macro runs in YACKM have a single solution and both Marshallian and Keynesian seem to get there fine. 

But because they use slightly different PID setups the speed of converge looks very different from sample runs.

###References

###Purpose

Test whether the speed of convergence is the same and if not test whether equalizing PID parameters make it the same


###Hypothesis

Speed of convergence is the same or can be made so easily

###Experiment

1000 runs each, plot histograms and do simple statistical tests if necessary.



###Code used

The dart code used is ```2015-02-08 convergenceSpeed.dart``` in the ```runs/yackm/experiments``` folder.

###Results

####Experiment 1
First I run the code without changing default, as of 2015-02-08. For Keynesian I have the following histogram:  
```{r , echo=FALSE,warning=FALSE}


test1<-read.csv(file.path(rawdataFolder,"convergeSpeed1.csv"))
test1$keynesian[test1$keynesian=="null"]<-NaN
test1$keynesian<-as.numeric(test1$keynesian)

hist(test1$keynesian,xlim=c(0,15000),main = "Keynesian Convergence Day")
failures<-sum(read.csv(file.path(rawdataFolder,"convergeSpeed1.csv"))$keynesian == "null")

```

There are also `r  failures ` failures. This means the failure rate is about `r failures/1000 *100`%

For marshallian:  
```{r , echo=FALSE,warning=FALSE}


test1<-read.csv(file.path(rawdataFolder,"convergeSpeed1.csv"))
isolated<-test1$marshallian
isolated[isolated=="null"]<-NaN
isolated<-as.numeric(isolated)

hist(isolated,xlim=c(0,15000),,main = "Marshallia nConvergence Day")
failures<-sum(read.csv(file.path(rawdataFolder,"convergeSpeed1.csv"))$marshallian == "null")
```

It never fails.

So with the default parameters it **Keynesian is a lot faster**.

####Experiment 2

Changed ```KEYNESIAN_QUOTA``` in the scenario declaration in ```model.dart```. While the default is critical inventory 100 and normal inventory 10, the keynesian quota originally went critical inventory 10, normal inventory 1.  
The unit tests ran fine, still a small % of failure for Keynesian setup though.

The results of this experiment are in ```convergeSpeed2.csv```
First I run the code without changing default, as of 2015-02-08. For Keynesian I have the following histogram:  
```{r , echo=FALSE,warning=FALSE}

cleanColumn<-function(column){
  
  column[column=="null"]<-NaN
  column<-as.numeric(as.character(column))
  return(column)
}

doubleHist<-function(filename)
  {
  
  data<-read.csv(file.path(rawdataFolder,filename))
  keynesian<-cleanColumn(data$keynesian)
  marshallian<-cleanColumn(data$marshallian)
  par(mfrow=c(2,1))
  hist(keynesian,xlim=c(0,15000),main = "Keynesian Convergence Day",breaks=seq.int(from=1,to=15000,by=200))
  hist(marshallian,xlim=c(0,15000),main = "Marshallian Convergence Day",breaks=seq.int(from=1,to=15000,by=200))
  par(mfrow=c(1,1))

  }

countFailures<-function(filename)
{
  return( c(
    sum(read.csv(file.path(rawdataFolder,filename))$marshallian == "null"),
    sum(read.csv(file.path(rawdataFolder,filename))$keynesian == "null")
    
    ))
  }

doubleHist("convergeSpeed2.csv")

```

As you can see, it doesn't matter much.

####Experiment 3

The ```PROFIT_MAXIMIZER_PRICING``` uses a PI multiplier in the Keynesian scenario of about 100. Without it tests fail. There was also an initial max price parameter of 27 which I killed (doesn't affect tests).  
What is going on is an extremely slow price adjustment:

```{r , echo=FALSE,warning=FALSE}
library(ggplot2)
library(reshape2)
library(gridExtra)
rawdataFolder<-
  file.path("~","code","lancaster","docs","yackm","rawdata")

plotMacro<-function(filename,correctPrice,correctQuantity,title)
{
  data <- read.csv(file.path(rawdataFolder,filename))
  
  market<-data.frame(quantity=data$quantity,price=data$price,day=1:length(data$quantity))
  
  #quantity
  plot1<-ggplot() + 
    geom_abline(intercept=correctQuantity,slope=0,aes(colour=factor("target")),size=1,alpha=0.5) +
    geom_line(data=market,aes(y=quantity,x=day)) + ylim(0,10) + theme_gray(20)
  #price
  plot2<-ggplot() + 
    geom_abline(intercept=correctPrice,slope=0,aes(colour=factor("target")),size=1,alpha=0.5) +
    geom_line(data=market,aes(y=price,x=day)) + ylim(0,75) + theme_gray(20)
  return(grid.arrange(plot1,plot2, ncol=1, main =title))
  
  
}

plotMacro("nomultiplierK.csv",20,5,"Keynesian Macro without multiplier")
```

On the other hand the marshallian quota also has a PImultiplier (defaulted to 100 as well). Taking it away also make the approach much slower:
```{r , echo=FALSE,warning=FALSE}
plotMacro("nomultiplierM.csv",20,5,"Marshallian Macro without multiplier")
```

####Experiment 4

Find the parameters for which price equal labor
$$
p = L
$$
Solution is just:
$$
\frac{2 \sqrt{\frac{4b^2}{a^2}}}{a} = \frac{4b^2}{a^2}
$$
Which, if we assume $a=1$ leads to $b=1$ and then:
>$$
\begin{align*}
 L &= 4 \\
 p &= 4 \\
 Y &= 1
\end{align*}
$$

But something really strange happen with this setup, the PI multiplier for the Keynesian maximizer (which sets prices) is just too high so you end up with very wavy results:
```{r , echo=FALSE,warning=FALSE}
plotMacro("overshooting.csv",1,4,"Keynesian Macro")
```
I know that's a multiplier issue because when I lower it from 100 to 50 I get no issues. Unfortunately Marshallian multiplier at 50 means that the results are usually in time, failing tests.

Interestingly, even though the multiplier is at 50, Keynesian is still faster:
```{r , echo=FALSE,warning=FALSE}
doubleHist("convergeSpeed3.csv")
```

####Experiment 5
I brought the multiplier back to 100, but this time I focus on initial price

####Experiment 6
So Marhsallian simulation uses buffer inventory while Keynesian seems to be using stockouts. I try then to run Marshall with stockout sales.  

```{r, echo=FALSE,warning=FALSE}
doubleHist("convergeSpeed4.csv")
```  

Not really any better.

####Experiment 7
Marshallian tries to equalize MB=MC by making the sigmoid of MB/MC equals to 1. The Keynesian moves price in the opposite direction. I did so previously by doing 1/sigmoid, let me just try to do -sigmoid.
```{r, echo=FALSE,warning=FALSE}
doubleHist("convergeSpeed5.csv")
```  

####Experiment 8
keeping the new sigmoid, try to focus again on $b=-1$ (fixed cost of production). Interestingly now some of the keynesian results are correct, although the majority still screws up
```{r, echo=FALSE,warning=FALSE}
doubleHist("convergeSpeed6.csv")
```  
Still not good enough, also a 40% failure rate which is unacceptable

####Experiment 9
Figuring out why it fails so damn much with Keynesian. It seems that at some point it just reaches escape velocity.

So there is a lot more outflow than inflow which pushes the labor PID to hire more workers
```{r, echo=FALSE,warning=FALSE}
ksales <- read.csv(file.path(rawdataFolder,"escapevelocity_sales.csv"))
khr <- read.csv(file.path(rawdataFolder,"escapevelocity_hr.csv"))
par(mfrow = c(2,1))
plot(ksales$outflow+ksales$stockouts-ksales$inflow,main="outflow-inflow")
plot(khr$quota,main="people hired")
```  
And since for these many workers $\text{MB} < \text{MC} $ you end up raising prices:
```{r, echo=FALSE,warning=FALSE}
par(mfrow = c(2,1))
plot(ksales$maximizer_ratio,main="MB/MC")
plot(ksales$offeredPrice,main="price")
```  

It is possible that keep hiring workers increase demand faster than it increases supply, keeping prices fixed. This is an issue because if the demand increases you answer by increasing workers in Keynesian world. Unfortunately that just starts the cycle once again.

Does it help if I increase the speed of the price pid? It would seem that increasing price faster than labor would break the error.

 


###Conclusion

The difference in speed doesn't seem caused primarily by parameter differences. I would like to make sure it's not simply a result of prices having to go all the way to 20 while labor has to go to 100 (or is it quantity going to 5?).

### Questions
* Why PI multiplier for experiment 4 needed to be lowered for Keynesian but not Marshallian (which in fact doesn't overshoot at 100 in the same market?)
* Why does it look like price in the Keynesian mode gets adjusted far more aggressively than labor in the Marshallian one besides being run by the same sticky maximizer?
* Could it be that escape velocity is self-adjusting when dropping Q/L but not necessarilly so when increasing it?

---------------------------------------

# Effect of a demand shock

### Date 
2015-02-10

###Background Information
At the present stage this seems like the most interesting question, what happens to labor, income and quantity when there is a demand shock. I know from the solution that if I lower demand by 1 the new solution is:

>$$
\begin{align*}
 L &= 64 \\
 p &= 16 \\
 Y &= 3
\end{align*}
$$ 

from
>$$
\begin{align*}
 L &= 100 \\
 p &= 20 \\
 Y &= 5
\end{align*}
$$

And I know from tests after a shock the new equilibrium is reached (I also know that the old equilibrium is returned to if the demand shock is then removed)

###References

###Purpose



###Hypothesis


###Experiment

####Experiment 1

Run the simulation for 20,000 days, at 10,000 the demand drops. The result is interesting for Keynesian world, but Marshallian is too slow. Marshallian: 
```{r,echo=FALSE,warning=FALSE}
rawdataFolder<-
  file.path("~","code","lancaster","docs","yackm","rawdata")
#Marshallian
marshall<-read.csv(file.path(rawdataFolder,"M_drop_gas.csv"))
plot(marshall$quantity,type="l")
abline(h = 5,lty=2,col="red")
abline(h= 3, lty=2, col = "blue")
```  
Keynesian:  
```{r,echo=FALSE,warning=FALSE}
rawdataFolder<-
  file.path("~","code","lancaster","docs","yackm","rawdata")
#Marshallian
keynesian<-read.csv(file.path(rawdataFolder,"K_drop_gas.csv"))
plot(keynesian$quantity,type="l")
abline(h = 5,lty=2,col="red")
abline(h= 3, lty=2, col = "blue")
```  
There is a large undershooting which is nice because I was expecting it.  This doesn't prove that the undershooting is impossible in the Marshallian world




####Experiment 2
I increase the Marshallian PI Multiplier to 1000, again this is mostly as a way to compare and contrast to similarly fast Keynesian. I get the following quantity:  
```{r,echo=FALSE,warning=FALSE}
rawdataFolder<-
  file.path("~","code","lancaster","docs","yackm","rawdata")
#Marshallian
marshall<-read.csv(file.path(rawdataFolder,"M_drop_gas_1000.csv"))
plot(marshall$quantity,type="l")
abline(h = 5,lty=2,col="red")
abline(h= 3, lty=2, col = "blue")
```
So there is no undershooting in quantity, and the overshooting at the beginning is really looks like a consequence of random initial price being so high:  
```{r,echo=FALSE,warning=FALSE}
plot(marshall$price,type="l")
abline(h = 20,lty=2,col="red")
abline(h= 16, lty=2, col = "blue")
```

Compare this with the Keynesian world where there is undershooting.

####Experiment 3
Keeping the Marshallian PI multiplier to 1000, here's what happens if the shock only lasts 5000 days.

```{r,echo=FALSE,warning=FALSE}
rawdataFolder<-
  file.path("~","code","lancaster","docs","yackm","rawdata")
#Marshallian
marshall<-read.csv(file.path(rawdataFolder,"M_cycle_gas_1000.csv"))
plot(marshall$quantity,type="l")
abline(h = 5,lty=2,col="red")
abline(h= 3, lty=2, col = "blue")
```  

No undershooting or overshooting in the Marshallian world.

```{r,echo=FALSE,warning=FALSE}
rawdataFolder<-
  file.path("~","code","lancaster","docs","yackm","rawdata")
#Marshallian
keynesian<-read.csv(file.path(rawdataFolder,"K_cycle_gas.csv"))
plot(keynesian$quantity,type="l")
abline(h = 5,lty=2,col="red")
abline(h= 3, lty=2, col = "blue")
```  
Plenty of overshooting in the Keynesian world though.



###Code used


###Results



###Conclusion

### Questions

---------------------------------------

# Convergence speed in a Micro-setting

### Date
2015-02-11

###Background Information
This follows from the previous convergence experiments in the macro-world. There seem to be too many moving parts there to really figure out what is the root cause of convergence speed. So I move to a simpler micro-example.

###References

check the experiment *Convergence Speed between Marshallian and Keynesian*

###Purpose
Simplify the scenario where Keynesian and Marshallian methods are employed to find the root cause of convergence speed divergence.


###Hypothesis
There is no difference in speed in the micro-world or if there is it is easily identifiable.

###Experiment
Does the difference in speed exist in the micro-econ world? To do I just replicate the linear demand-linear supply model that has a nice equilibrium:
>$$
\begin{align*}
 L &= 50 \\
 p &= 50 \\
 Y &= 50
\end{align*}
$$
Like the micro-tests but with an infinitely elastic labor supply at $w=50$. The convergence speed should be exactly the same:

```{r, echo=FALSE,warning=FALSE}
doubleHist("micro_converge_speed.csv")
```  

And weirdly the opposite happens: Marshallian convergence speed is a lot faster!

Now if I plot 

```{r, echo=FALSE,warning=FALSE}
ksales <- head(read.csv(file.path(rawdataFolder,"K_micro_sales.csv")),n=2000)
khr <- head(read.csv(file.path(rawdataFolder,"K_micro_hr.csv")),n=2000)
msales <- head(read.csv(file.path(rawdataFolder,"M_micro_sales.csv")),n=2000)
mhr <- head(read.csv(file.path(rawdataFolder,"M_micro_hr.csv")),n=2000)
par(mfrow = c(3,2))
plot(ksales$outflow+ksales$stockouts-ksales$inflow,main="Keynesian outflow-inflow",type="l")
plot(msales$outflow+msales$stockouts-msales$inflow,main="Marshallian outflow-inflow",type="l")
plot(khr$quota,main="Keynesian L",type="l")
plot(mhr$quota,main="Marshallian L",type="l")
plot(ksales$offeredPrice,main="Keynesian P",type="l")
plot(msales$offeredPrice,main="Marshallian P",type="l")
par(mfrow = c(1,1))
```  

Marshallian L and Keynesian P should be run by the same kind of PI controller with the same parameters, but it looks like the Marshallian L adapts a bit faster in this case than the Keynesian P. (notice also the difference in starting point which should give advantage to the Keynesian).

If I look at the PI maximizers themselves:  

```{r, echo=FALSE,warning=FALSE}
par(mfrow = c(2,1))
#flip the keynesian efficiency since it goes in reverse
plot(-ksales$maximizer_efficiency,type="l", main="Keynesian PI input",ylim=c(-1,1))
abline(h=0.5,col="red",lty=2)
plot(mhr$maximizer_efficiency,type="l", main="Marshallian PI input",ylim=c(0,1))
abline(h=0.5,col="red",lty=2)
```

So it seems like the Marshallian PI is faster simply because we put larger inputs in it. Is it because of Keynesian starting P is usually closer to equilibrium than Marshallian starting L which is always 1? To study this I try again 1000 runs by making Marshallian L also randomly distributed. The result?

```{r, echo=FALSE,warning=FALSE}
doubleHist("micro_converge_speed2.csv")
```  

Mega-sucess! The success remain if I also randomize the initial L of the Keynesian PI (which adapts fast):

```{r, echo=FALSE,warning=FALSE}
doubleHist("micro_converge_speed3.csv")
```




###Code used

The main code is ```2015-02-11_microspeed.dartre```

###Results

* The convergence speed difference can be attributed entirely to initial conditions rather than PI parameters (at least in micro)
* Starting closer to equilibrium doesn't mean you will achieve it quickly. Quite the opposite in this example.

###Conclusion

### Questions
* Can I generalize this back to macro?


---------------------------------------

#Convergence Speed after a drop

### Date 

2015-02-11

###Background Information

I know that initial prices/targets matter when it comes to convergence speed in micro so I assume they matter in macro as well. Let's try!

###References

###Purpose

The best way to make sure Marshallian and Keynesian agents start the same is to make them go to an equilibrium then shock the demand and see how much it takes to go to a new equilibrium.


###Hypothesis

Either there is no difference in speed or if there is it must be irreducibile (that is it is inherent in the way they adjust rather than differences in parameters)

###Experiment

Make the production equation look like this:  
$$
Y = 0.5 \sqrt L - 1
$$

This gives rises to the following macro equilibrium 
>$$
\begin{align*}
 L &= 16 \\
 p &= 16 \\
 Y &= 1
\end{align*}
$$

Then I lower the demand by 0.2 when shocked and I get:
\begin{align*}
 L &= 10.24 \\
 p &= 12.8 \\
 Y &= 0.6
\end{align*}
$$

```{r, echo=FALSE,warning=FALSE}
doubleHist("shock_converge_speed.csv")
```

Sample runs look like this:

(notice that the speed difference is very different from previous experiments because $l$ and $p$ are closer)


###Code used


###Results



###Conclusion

### Questions



---------------------------------------

#title

### Date 

###Background Information


###References

###Purpose



###Hypothesis


###Experiment




###Code used


###Results



###Conclusion

### Questions

